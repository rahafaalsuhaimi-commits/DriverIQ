{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "JPkMXArpecu8",
        "ojJYmU0sewSc",
        "QkdGKYunvPyg",
        "0yHcj799GueL"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahafaalsuhaimi-commits/DriverIQ/blob/main/Accident_Prediction_DriverIq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lib"
      ],
      "metadata": {
        "id": "JPkMXArpecu8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjkZ9wp3eOAG",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "pip install tnkeeh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install arabert"
      ],
      "metadata": {
        "id": "2WVympVuFtS5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "7ZGsLPsNF2ZV",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Arabic-Stopwords"
      ],
      "metadata": {
        "id": "FYbf_x4PGBoN",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install Tashaphyne"
      ],
      "metadata": {
        "id": "bEyneIPFGcXh",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyArabic"
      ],
      "metadata": {
        "id": "hKfIPUiGGeKl",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji"
      ],
      "metadata": {
        "id": "7K0kqLI_GgIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openpyxl"
      ],
      "metadata": {
        "id": "y2rxV8aiGgXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn"
      ],
      "metadata": {
        "id": "Y6lSG4-tGO9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nltk # Text libarary\n",
        "# nltk.download('stopwords')\n",
        "import string\n",
        "import re\n",
        "import regex\n",
        "import emoji\n",
        "from nltk.corpus import stopwords\n",
        "import arabicstopwords.arabicstopwords as stp #more range of arabic stop words\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "import pyarabic.araby as araby\n",
        "from tashaphyne.stemming import ArabicLightStemmer\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.svm import OneClassSVM\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.svm import SVC"
      ],
      "metadata": {
        "id": "5-iXWRicF-8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "VGBohyEMejEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "utJ3HkjyynxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "ojJYmU0sewSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('data.csv')"
      ],
      "metadata": {
        "id": "4ghYzqWs51X5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "UnVwBNxsJaRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(50)"
      ],
      "metadata": {
        "id": "do9PUO7Le-T0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# preprocessing"
      ],
      "metadata": {
        "id": "QkdGKYunvPyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove numbers and periods\n",
        "data['Text'] = data['Text'].str.replace(r'[\\d.]', '', regex=True)"
      ],
      "metadata": {
        "id": "0iVQlKFQ3LwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "E2xF2x2x3cpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Text'] = data['Text'].apply(lambda text: Removing_non_arabic(text))\n",
        "data['Text'] = data['Text'].apply(lambda text: Removing_punctuations(text))\n",
        "data['Text'] = data['Text'].apply(lambda text: remove_emoji(text))\n",
        "data = data[data['Text'].str.split().str.len() > 1]"
      ],
      "metadata": {
        "id": "Pow9P8JG7sR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.dropna(subset=['Text'], inplace =True)"
      ],
      "metadata": {
        "id": "UhX3TwJQvRdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.duplicated().sum()"
      ],
      "metadata": {
        "id": "eToQIAMIJ_ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "cI73F-anHv35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arabic_stopwords = stopwords.words(\"arabic\")"
      ],
      "metadata": {
        "id": "siWjpldoHrlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(text):\n",
        "    Text=[i for i in str(text).split() if i not in arabic_stopwords]\n",
        "    return \" \".join(Text)\n",
        "\n",
        "def ISRI_Stemmer(text):\n",
        "    #making an object\n",
        "    stemmer = ISRIStemmer()\n",
        "\n",
        "    #stemming each word\n",
        "    text = stemmer.stem(text)\n",
        "    text = stemmer.pre32(text)\n",
        "    text = stemmer.suf32(text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def Snowball_stemmer(text):\n",
        "    text = text.split()\n",
        "    stemmer = SnowballStemmer(\"arabic\")\n",
        "    text=[stemmer.stem(y) for y in text]\n",
        "    return \" \" .join(text)\n",
        "\n",
        "def Arabic_Light_Stemmer(text):\n",
        "    Arabic_Stemmer = ArabicLightStemmer()\n",
        "    text=[Arabic_Stemmer.light_stem(y) for y in text.split()]\n",
        "\n",
        "    return \" \" .join(text)\n",
        "\n",
        "def normalizeArabic(text):\n",
        "    text = text.strip()\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ؤ\", \"ء\", text)\n",
        "    text = re.sub(\"ئ\", \"ء\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "\n",
        "    #remove repetetions\n",
        "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
        "    text = text.replace('وو', 'و')\n",
        "    text = text.replace('يي', 'ي')\n",
        "    text = text.replace('ييي', 'ي')\n",
        "    text = text.replace('اا', 'ا')\n",
        "\n",
        "    ## remove extra whitespace\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "\n",
        "    # Remove longation\n",
        "    text = re.sub(r'(.)\\1+', r\"\\1\\1\", text)\n",
        "\n",
        "    #Strip vowels from a text, include Shadda.\n",
        "    text = araby.strip_tashkeel(text)\n",
        "\n",
        "    #Strip diacritics from a text, include harakats and small lettres The striped marks are\n",
        "    text = araby.strip_diacritics(text)\n",
        "    text=''.join([i for i in text if not i.isdigit()])\n",
        "    return text\n",
        "\n",
        "def Removing_non_arabic(text):\n",
        "    text = re.sub('[A-Za-z]+',' ',text)\n",
        "    return text\n",
        "\n",
        "def Removing_numbers(text):\n",
        "    text=''.join([i for i in text if not i.isdigit()])\n",
        "    return text\n",
        "\n",
        "def Removing_punctuations(text):\n",
        "    ## Remove punctuations\n",
        "    text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,،-./:;<=>؟?@[\\]^_`{|}~\"\"\"), ' ', text)\n",
        "    text = text.replace('؛',\"\", )\n",
        "\n",
        "    ## remove extra whitespace\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    text =  \" \".join(text.split())\n",
        "    return text.strip()\n",
        "\n",
        "def remove_emoji(string):\n",
        "\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string).strip()\n",
        "\n",
        "\n",
        "def remove_extra_Space(text):\n",
        "    ## remove extra whitespace\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    return  \" \".join(text.split())\n"
      ],
      "metadata": {
        "id": "8VBWeZKLEAab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.Text=data.Text.apply(lambda text : remove_stop_words(text))"
      ],
      "metadata": {
        "id": "9hGLJmL0DOUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.Text=data.Text.apply(lambda text : Removing_non_arabic(text))\n",
        "#data.Text=data.Text.apply(lambda text : normalizeArabic(text))\n",
        "data.Text=data.Text.apply(lambda text : Removing_punctuations(text))\n",
        "#data.Text=data.Text.apply(lambda text : Arabic_Light_Stemmer(text))"
      ],
      "metadata": {
        "id": "yKyNXg-RH1pd",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "usRU7imuIt4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_csv('new_cleand_data.csv', index = False)"
      ],
      "metadata": {
        "id": "943RepvLI1xV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentence Transformer"
      ],
      "metadata": {
        "id": "0yHcj799GueL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4a99100"
      },
      "source": [
        "data = pd.read_excel('balanced_cleand_data.xlsx')\n",
        "X = data['Text']\n",
        "y = data['label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b2dd11a"
      },
      "source": [
        "model = SentenceTransformer(\"aubmindlab/bert-base-arabertv2\") #tokenaization\n",
        "text_embeddings = model.encode(X.to_list())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b738a063"
      },
      "source": [
        "# Binary SVM\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = data[\"label\"].tolist()"
      ],
      "metadata": {
        "id": "-J32o_uqAAAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "accs, precs, recs, f1s = [], [], [], []\n",
        "cm_total = np.zeros((2, 2), dtype=int)\n",
        "\n",
        "for train_idx, test_idx in cv.split(text_embeddings, labels):\n",
        "    X_train, X_test = text_embeddings[train_idx], text_embeddings[test_idx]\n",
        "    y_train, y_test = np.array(labels)[train_idx], np.array(labels)[test_idx]\n",
        "\n",
        "    svm = SVC(kernel=\"linear\")\n",
        "    svm.fit(X_train, y_train)\n",
        "    y_pred = svm.predict(X_test)\n",
        "\n",
        "    accs.append(accuracy_score(y_test, y_pred))\n",
        "    precs.append(precision_score(y_test, y_pred))\n",
        "    recs.append(recall_score(y_test, y_pred))\n",
        "    f1s.append(f1_score(y_test, y_pred))\n",
        "\n",
        "    cm_total += confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
        "\n",
        "print(\"Mean Accuracy:\", np.mean(accs))\n",
        "print(\"Mean Precision:\", np.mean(precs))\n",
        "print(\"Mean Recall:\", np.mean(recs))\n",
        "print(\"Mean F1:\", np.mean(f1s))"
      ],
      "metadata": {
        "id": "qbhfs4tLACRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm_total, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"Not Accident\", \"Accident\"],\n",
        "            yticklabels=[\"Not Accident\", \"Accident\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix for SVM model\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rN7sWZDWA9tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for testing using generated data"
      ],
      "metadata": {
        "id": "TBVpogFFXQsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gen_data = pd.read_excel('GenData.xlsx')\n",
        "\n",
        "# Encoding using SentenceTransformer model\n",
        "gen_text_embeddings = model.encode(gen_data['Text'].to_list())\n",
        "\n",
        "#predictions\n",
        "gen_predictions = svm.predict(gen_text_embeddings)\n",
        "\n",
        "# Ato see the prediction on a Data Frame\n",
        "gen_data['Predicted_Label'] = gen_predictions\n",
        "\n",
        "# Compare the predicted and actuall labels\n",
        "true_labels = gen_data['True_Label']\n",
        "\n",
        "accuracy = accuracy_score(true_labels, gen_predictions)\n",
        "precision = precision_score(true_labels, gen_predictions)\n",
        "recall = recall_score(true_labels, gen_predictions)\n",
        "f1 = f1_score(true_labels, gen_predictions)\n",
        "\n",
        "print(f\"Accuracy on GenData: {accuracy:.4f}\")\n",
        "print(f\"Precision on GenData: {precision:.4f}\")\n",
        "print(f\"Recall on GenData: {recall:.4f}\")\n",
        "print(f\"F1 Score on GenData: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "bTqcDiBEA2yU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_data.to_excel('GenData_with_predictions.xlsx', index=False)"
      ],
      "metadata": {
        "id": "vCE4S3a8X0SV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}